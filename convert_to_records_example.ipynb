{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Refactored from Daniil's blog: TFRecords Guide\n",
    "# http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "from scipy.misc import imread, imresize\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for downloading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from https://github.com/tensorflow/models/blob/master/slim/datasets/dataset_utils.py\n",
    "def download_and_uncompress_tarball(tarball_url, dataset_dir):\n",
    "    \"\"\"Downloads the `tarball_url` and uncompresses it locally.\n",
    "    Args:\n",
    "    tarball_url: The URL of a tarball file.\n",
    "    dataset_dir: The directory where the temporary files are stored.\n",
    "    \"\"\"\n",
    "    filename = tarball_url.split('/')[-1]\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "\n",
    "    def _progress(count, block_size, total_size):\n",
    "        sys.stdout.write('\\r>> Downloading %s %.1f%%' % \n",
    "                         (filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "        sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions and main function for converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    \n",
    "def main(_):\n",
    "    # Download dataset (Caltech101)\n",
    "    if not os.path.isdir(FLAGS.directory):\n",
    "        parentdir = os.path.join(*FLAGS.directory.split('/')[:-1])\n",
    "        os.makedirs(parentdir)\n",
    "        download_and_uncompress_tarball(FLAGS.fileurl, parentdir)\n",
    "        \n",
    "    \n",
    "    # Load Custom Dat (Caltech101)\n",
    "    path = FLAGS.directory\n",
    "    valid_exts = ['.jpg', '.gif', '.png', '.jpeg']\n",
    "    print('%d categories in %s' % (len(os.listdir(path)), path))\n",
    "    \n",
    "    categories = sorted(os.listdir(path))\n",
    "    num_categ = len(categories)\n",
    "    \n",
    "    imgnames = []\n",
    "    labels = []\n",
    "    for label, category in enumerate(categories):\n",
    "        filelist = os.listdir(os.path.join(path, category))\n",
    "        imglist = []\n",
    "        for f in filelist:\n",
    "            ext = os.path.splitext(f)[-1]\n",
    "            if ext.lower() not in valid_exts:\n",
    "                continue\n",
    "            imglist.append(f)\n",
    "        imgnames += imglist\n",
    "        labels += [label]*len(imglist)\n",
    "        print('[%3d/%3d] %d %s images are found' % (label+1, num_categ,\n",
    "                                                    len(imglist), category))\n",
    "\n",
    "    numfiles = len(labels)\n",
    "    ratio = FLAGS.trainset_ratio\n",
    "    idxrand = np.random.permutation(numfiles)\n",
    "    idxtrain = idxrand[:int(ratio*numfiles)]\n",
    "    idxtest = idxrand[int(ratio*numfiles):]\n",
    "    \n",
    "    tfrecord_fname_train = 'custom_train.tfrecords'\n",
    "    tfrecord_fname_test = 'custom_test.tfrecords'\n",
    "    \n",
    "    writer_train = tf.python_io.TFRecordWriter(os.path.join(FLAGS.directory, '../', tfrecord_fname_train))\n",
    "    writer_test = tf.python_io.TFRecordWriter(os.path.join(FLAGS.directory, '../', tfrecord_fname_test))\n",
    "    \n",
    "    disp_step = 400\n",
    "    print('\\nWriting \\\"%s\\\" start' % tfrecord_fname_train)\n",
    "    for itr, idx in enumerate(idxtrain):\n",
    "        label = labels[idx]\n",
    "        imgpath = os.path.join(path, categories[label], imgnames[idx])\n",
    "        image = np.array(Image.open(imgpath))\n",
    "        \n",
    "        try:\n",
    "            depth = image.shape[2]\n",
    "        except:\n",
    "            image = np.tile(np.expand_dims(image, axis=2), [1,1,3])\n",
    "            depth = image.shape[2]\n",
    "            \n",
    "        image = imresize(image, [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "        image_str = image.tostring()\n",
    "\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(IMAGE_HEIGHT),\n",
    "            'width': _int64_feature(IMAGE_WIDTH),\n",
    "            'depth': _int64_feature(depth),\n",
    "            'image_raw': _bytes_feature(image_str),\n",
    "            'label': _int64_feature(label)\n",
    "        }))\n",
    "        writer_train.write(example.SerializeToString())\n",
    "        \n",
    "        if (itr+1)% disp_step == 0 or (itr+1) == idxtrain.shape[0]:\n",
    "            print('[%3d/%3d] train data has been written' % (itr+1,\n",
    "                                                             idxtrain.shape[0]))\n",
    "    \n",
    "    print('\\nWriting \\\"%s\\\" start' % tfrecord_fname_test)\n",
    "    for itr, idx in enumerate(idxtest):\n",
    "        label = labels[idx]\n",
    "        imgpath = os.path.join(path, categories[label], imgnames[idx])\n",
    "        image = np.array(Image.open(imgpath))\n",
    "\n",
    "        try:\n",
    "            depth = image.shape[2]\n",
    "        except:\n",
    "            image = np.tile(np.expand_dims(image, axis=2), [1,1,3])\n",
    "            depth = image.shape[2]\n",
    "            \n",
    "        image = imresize(image, [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "        image_str = image.tostring()\n",
    "\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(IMAGE_HEIGHT),\n",
    "            'width': _int64_feature(IMAGE_WIDTH),\n",
    "            'depth': _int64_feature(depth),\n",
    "            'image_raw': _bytes_feature(image_str),\n",
    "            'label': _int64_feature(label)\n",
    "        }))\n",
    "        writer_test.write(example.SerializeToString())\n",
    "        \n",
    "        if (itr+1) % disp_step == 0 or (itr+1) == idxtest.shape[0]:\n",
    "            print('[%3d/%3d] test data has been written' % (itr+1,\n",
    "                                                            idxtest.shape[0]))\n",
    "\n",
    "    writer_train.close()\n",
    "    writer_test.close()\n",
    "    print('Done')              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading 101_ObjectCategories.tar.gz 100.0%\n",
      "Successfully downloaded 101_ObjectCategories.tar.gz 131740031 bytes.\n",
      "102 categories in tmp/101_ObjectCategories\n",
      "[  1/102] 467 BACKGROUND_Google images are found\n",
      "[  2/102] 435 Faces images are found\n",
      "[  3/102] 435 Faces_easy images are found\n",
      "[  4/102] 200 Leopards images are found\n",
      "[  5/102] 798 Motorbikes images are found\n",
      "[  6/102] 55 accordion images are found\n",
      "[  7/102] 800 airplanes images are found\n",
      "[  8/102] 42 anchor images are found\n",
      "[  9/102] 42 ant images are found\n",
      "[ 10/102] 47 barrel images are found\n",
      "[ 11/102] 54 bass images are found\n",
      "[ 12/102] 46 beaver images are found\n",
      "[ 13/102] 33 binocular images are found\n",
      "[ 14/102] 128 bonsai images are found\n",
      "[ 15/102] 98 brain images are found\n",
      "[ 16/102] 43 brontosaurus images are found\n",
      "[ 17/102] 85 buddha images are found\n",
      "[ 18/102] 91 butterfly images are found\n",
      "[ 19/102] 50 camera images are found\n",
      "[ 20/102] 43 cannon images are found\n",
      "[ 21/102] 123 car_side images are found\n",
      "[ 22/102] 47 ceiling_fan images are found\n",
      "[ 23/102] 59 cellphone images are found\n",
      "[ 24/102] 62 chair images are found\n",
      "[ 25/102] 107 chandelier images are found\n",
      "[ 26/102] 47 cougar_body images are found\n",
      "[ 27/102] 69 cougar_face images are found\n",
      "[ 28/102] 73 crab images are found\n",
      "[ 29/102] 70 crayfish images are found\n",
      "[ 30/102] 50 crocodile images are found\n",
      "[ 31/102] 51 crocodile_head images are found\n",
      "[ 32/102] 57 cup images are found\n",
      "[ 33/102] 67 dalmatian images are found\n",
      "[ 34/102] 52 dollar_bill images are found\n",
      "[ 35/102] 65 dolphin images are found\n",
      "[ 36/102] 68 dragonfly images are found\n",
      "[ 37/102] 75 electric_guitar images are found\n",
      "[ 38/102] 64 elephant images are found\n",
      "[ 39/102] 53 emu images are found\n",
      "[ 40/102] 64 euphonium images are found\n",
      "[ 41/102] 85 ewer images are found\n",
      "[ 42/102] 67 ferry images are found\n",
      "[ 43/102] 67 flamingo images are found\n",
      "[ 44/102] 45 flamingo_head images are found\n",
      "[ 45/102] 34 garfield images are found\n",
      "[ 46/102] 34 gerenuk images are found\n",
      "[ 47/102] 51 gramophone images are found\n",
      "[ 48/102] 99 grand_piano images are found\n",
      "[ 49/102] 100 hawksbill images are found\n",
      "[ 50/102] 42 headphone images are found\n",
      "[ 51/102] 54 hedgehog images are found\n",
      "[ 52/102] 88 helicopter images are found\n",
      "[ 53/102] 80 ibis images are found\n",
      "[ 54/102] 31 inline_skate images are found\n",
      "[ 55/102] 64 joshua_tree images are found\n",
      "[ 56/102] 86 kangaroo images are found\n",
      "[ 57/102] 114 ketch images are found\n",
      "[ 58/102] 61 lamp images are found\n",
      "[ 59/102] 81 laptop images are found\n",
      "[ 60/102] 78 llama images are found\n",
      "[ 61/102] 41 lobster images are found\n",
      "[ 62/102] 66 lotus images are found\n",
      "[ 63/102] 43 mandolin images are found\n",
      "[ 64/102] 40 mayfly images are found\n",
      "[ 65/102] 87 menorah images are found\n",
      "[ 66/102] 32 metronome images are found\n",
      "[ 67/102] 76 minaret images are found\n",
      "[ 68/102] 55 nautilus images are found\n",
      "[ 69/102] 35 octopus images are found\n",
      "[ 70/102] 39 okapi images are found\n",
      "[ 71/102] 47 pagoda images are found\n",
      "[ 72/102] 38 panda images are found\n",
      "[ 73/102] 45 pigeon images are found\n",
      "[ 74/102] 53 pizza images are found\n",
      "[ 75/102] 34 platypus images are found\n",
      "[ 76/102] 57 pyramid images are found\n",
      "[ 77/102] 82 revolver images are found\n",
      "[ 78/102] 59 rhino images are found\n",
      "[ 79/102] 49 rooster images are found\n",
      "[ 80/102] 40 saxophone images are found\n",
      "[ 81/102] 63 schooner images are found\n",
      "[ 82/102] 39 scissors images are found\n",
      "[ 83/102] 84 scorpion images are found\n",
      "[ 84/102] 57 sea_horse images are found\n",
      "[ 85/102] 35 snoopy images are found\n",
      "[ 86/102] 64 soccer_ball images are found\n",
      "[ 87/102] 45 stapler images are found\n",
      "[ 88/102] 86 starfish images are found\n",
      "[ 89/102] 59 stegosaurus images are found\n",
      "[ 90/102] 64 stop_sign images are found\n",
      "[ 91/102] 35 strawberry images are found\n",
      "[ 92/102] 85 sunflower images are found\n",
      "[ 93/102] 49 tick images are found\n",
      "[ 94/102] 86 trilobite images are found\n",
      "[ 95/102] 75 umbrella images are found\n",
      "[ 96/102] 239 watch images are found\n",
      "[ 97/102] 37 water_lilly images are found\n",
      "[ 98/102] 59 wheelchair images are found\n",
      "[ 99/102] 34 wild_cat images are found\n",
      "[100/102] 56 windsor_chair images are found\n",
      "[101/102] 39 wrench images are found\n",
      "[102/102] 60 yin_yang images are found\n",
      "\n",
      "Writing \"custom_train.tfrecords\" start\n",
      "[400/6400] train data has been written\n",
      "[800/6400] train data has been written\n",
      "[1200/6400] train data has been written\n",
      "[1600/6400] train data has been written\n",
      "[2000/6400] train data has been written\n",
      "[2400/6400] train data has been written\n",
      "[2800/6400] train data has been written\n",
      "[3200/6400] train data has been written\n",
      "[3600/6400] train data has been written\n",
      "[4000/6400] train data has been written\n",
      "[4400/6400] train data has been written\n",
      "[4800/6400] train data has been written\n",
      "[5200/6400] train data has been written\n",
      "[5600/6400] train data has been written\n",
      "[6000/6400] train data has been written\n",
      "[6400/6400] train data has been written\n",
      "\n",
      "Writing \"custom_test.tfrecords\" start\n",
      "[400/2744] test data has been written\n",
      "[800/2744] test data has been written\n",
      "[1200/2744] test data has been written\n",
      "[1600/2744] test data has been written\n",
      "[2000/2744] test data has been written\n",
      "[2400/2744] test data has been written\n",
      "[2744/2744] test data has been written\n",
      "Done\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2855: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--fileurl',\n",
    "        type=str,\n",
    "        default='http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz',\n",
    "        help='Dataset url'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--directory',\n",
    "        type=str,\n",
    "        default='tmp/101_ObjectCategories',\n",
    "        help='Directory to read data files and write the converted result'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--trainset_ratio',\n",
    "        type=float,\n",
    "        default=0.7,\n",
    "        help='Ratio of trainset to whole data'\n",
    "    )\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
